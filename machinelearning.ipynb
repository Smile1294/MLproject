{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:  Index(['lex_liwc_Clout', 'lex_liwc_Authentic', 'lex_liwc_Tone', 'lex_liwc_i',\n",
      "       'lex_liwc_posemo', 'lex_liwc_negemo', 'lex_liwc_anx', 'lex_liwc_social',\n",
      "       'lex_dal_min_pleasantness', 'sentiment'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "converting documents to features: 100%|██████████| 3553/3553 [00:12<00:00, 285.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "{'test_accuracy': 0.7116736990154712, 'test_roc_auc': 0.7080766652835848, 'runtime': 0.10447049140930176}\n",
      "LogisticRegression\n",
      "{'test_accuracy': 0.6216596343178622, 'test_roc_auc': 0.621453193322907, 'runtime': 0.008110523223876953}\n",
      "KNeighborsClassifier\n",
      "{'test_accuracy': 0.530239099859353, 'test_roc_auc': 0.5345783728575533, 'runtime': 0.06361985206604004}\n",
      "DecisionTreeClassifier\n",
      "{'test_accuracy': 0.6033755274261603, 'test_roc_auc': 0.6010700264913345, 'runtime': 0.00831913948059082}\n",
      "SVC\n",
      "{'test_accuracy': 0.5555555555555556, 'test_roc_auc': 0.548438447543966, 'runtime': 0.7769894599914551}\n",
      "GaussianNB\n",
      "{'test_accuracy': 0.4711673699015471, 'test_roc_auc': 0.5126113114806422, 'runtime': 0.01600193977355957}\n",
      "AdaBoostClassifier\n",
      "{'test_accuracy': 0.7130801687763713, 'test_roc_auc': 0.7122179311225304, 'runtime': 0.12847590446472168}\n",
      "GradientBoostingClassifier\n",
      "{'test_accuracy': 0.7144866385372715, 'test_roc_auc': 0.711950623982637, 'runtime': 0.0077402591705322266}\n",
      "MLPClassifier\n",
      "{'test_accuracy': 0.540084388185654, 'test_roc_auc': 0.5384124349685615, 'runtime': 0.008022785186767578}\n",
      "XGBClassifier\n",
      "{'test_accuracy': 0.7158931082981715, 'test_roc_auc': 0.7137579394210207, 'runtime': 0.024254322052001953}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "import time\n",
    "import tqdm\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import pad_sequences\n",
    "import transformers\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, VarianceThreshold, SelectFromModel\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"./dreaddit-train.csv\")\n",
    "test = pd.read_csv(\"./dreaddit-test.csv\")\n",
    "\n",
    "train['subreddit'] = le.fit_transform(train['subreddit'])\n",
    "test['subreddit'] = le.transform(test['subreddit'])\n",
    "merged = train.append(test)\n",
    "\n",
    "\n",
    "#############################################\n",
    "feature_selection = 1 #1 NEW FEATURE SELECTION #2 HOMEWORK FEATURE SELECTION\\\n",
    "rand_st = 1\n",
    "#############################################\n",
    "\n",
    "#creating a function\n",
    "def func_tokenizer(tokenizer_name, docs):\n",
    "    features = []\n",
    "    for doc in tqdm.tqdm(docs, desc = 'converting documents to features'):\n",
    "        tokens = tokenizer_name.tokenize(doc)\n",
    "        ids = tokenizer_name.convert_tokens_to_ids(tokens)\n",
    "        features.append(ids)\n",
    "    return features\n",
    "\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(('RandomForestClassifier', RandomForestClassifier()))\n",
    "    models.append(('LogisticRegression', LogisticRegression()))\n",
    "    models.append(('KNeighborsClassifier', KNeighborsClassifier()))\n",
    "    models.append(('DecisionTreeClassifier', DecisionTreeClassifier()))\n",
    "    models.append(('SVC', SVC()))\n",
    "    models.append(('GaussianNB', GaussianNB()))\n",
    "    models.append(('AdaBoostClassifier', AdaBoostClassifier()))\n",
    "    models.append(('GradientBoostingClassifier', GradientBoostingClassifier()))\n",
    "    models.append(('MLPClassifier', MLPClassifier()))\n",
    "    models.append(('XGBClassifier', XGBClassifier()))\n",
    "    return models\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "#Make X have everything except label\n",
    "new_copy_train = merged.copy()\n",
    "X,y = new_copy_train.drop(['label','post_id','id','subreddit','sentence_range','text'], axis=1), new_copy_train['label']\n",
    "\n",
    "selected_features = []\n",
    "#My feature selection\n",
    "if feature_selection == 1:\n",
    "    # Feature selection\n",
    "    selector = SelectKBest(f_classif, k=10)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "\n",
    "    # Get the support mask\n",
    "    mask = selector.get_support()\n",
    "\n",
    "    # Get the selected features\n",
    "    selected_features = X.columns[mask]\n",
    "\n",
    "    # Print the selected features\n",
    "    print(\"Selected Features: \", selected_features)\n",
    "if feature_selection == 2:\n",
    "    sel = VarianceThreshold(threshold=0.5)\n",
    "    fit_mod=sel.fit(X)\n",
    "    fitted=sel.transform(X)\n",
    "    sel_idx=fit_mod.get_support()\n",
    "    #Get lists of selected and non-selected features (names and indexes)\n",
    "    header = list(X.columns.values)\n",
    "    temp=[]\n",
    "    temp_idx=[]\n",
    "    temp_del=[]\n",
    "    for i in range(len(sel_idx)):\n",
    "        if sel_idx[i]==True:\n",
    "            temp.append(header[i])\n",
    "            temp_idx.append(i)\n",
    "        else:\n",
    "            temp_del.append(i)\n",
    "    selected_features=temp\n",
    "    sel_features_idx=temp_idx\n",
    "    del_features=temp_del\n",
    "    print(\"Selected Features: \", selected_features)\n",
    "    print(\"Deleted Features: \", del_features)\n",
    "\n",
    "\n",
    "# use selected features to fit the model and add back text column and tokenized text column\n",
    "X['text'] = train['text']\n",
    "bert_features = func_tokenizer(tokenizer, X['text'])\n",
    "bert_trg = pad_sequences(bert_features, maxlen=500, dtype='long', truncating='post', padding='post')\n",
    "X = pd.DataFrame(bert_trg)\n",
    "\n",
    "#add back the selected features\n",
    "for i in range(len(selected_features)):\n",
    "    X[selected_features[i]] = X_new[:,i]\n",
    "\n",
    "\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "\n",
    "scorers = {'Accuracy': 'accuracy', 'roc_auc': 'roc_auc'}                                                                                                                \n",
    "model_params = {\n",
    "    'RandomForestClassifier': {'n_estimators': 250, 'max_depth': None, 'min_samples_split': 3, 'criterion': 'entropy', 'random_state': rand_st},\n",
    "    'DecisionTreeClassifier': {'max_depth': None, 'min_samples_split': 3, 'criterion': 'entropy', 'random_state': rand_st},\n",
    "    'SVC': {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale', 'random_state': rand_st},\n",
    "    'KNeighborsClassifier': {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto'},\n",
    "    'LogisticRegression': {'penalty': 'l2', 'C': 1.0, 'solver': 'sag', 'random_state': rand_st, 'max_iter': 350},\n",
    "    'GaussianNB': {'var_smoothing': 1e-09},\n",
    "    'AdaBoostClassifier': {'n_estimators': 100, 'random_state': rand_st},\n",
    "    'GradientBoostingClassifier': {'n_estimators': 100, 'max_depth': 3, 'random_state': rand_st},\n",
    "    'MLPClassifier': {'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0001, 'batch_size': 'auto', 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'power_t': 0.5, 'max_iter': 200, 'shuffle': True, 'random_state': rand_st, 'tol': 0.0001, 'verbose': False, 'warm_start': False, 'momentum': 0.9, 'nesterovs_momentum': True, 'early_stopping': False, 'validation_fraction': 0.1, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08},\n",
    "    'XGBClassifier': {'n_estimators': 250, 'max_depth': 3, 'learning_rate': 0.1, 'random_state': rand_st}\n",
    "}\n",
    "model_results = []\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import for auc score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "scores={}\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#UPDATE THIS VALUE TO 1 IN ORDER TO RUN CROSS VALIDATION\n",
    "cross_val = 0\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "if cross_val == 0:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    for models in get_models():\n",
    "        name, clf = models\n",
    "        clf.set_params(**model_params[name])\n",
    "        clf.fit(X_train, y_train)    \n",
    "        start_ts=time.time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        test_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        scores['test_accuracy'] = test_accuracy\n",
    "        scores['test_roc_auc'] = test_roc_auc\n",
    "        scores['runtime'] = time.time()-start_ts\n",
    "        print(name)\n",
    "        print(scores)\n",
    "        model_results.append((name, scores))\n",
    "else:\n",
    "    for models in get_models():\n",
    "        name, clf = models\n",
    "        clf.set_params(**model_params[name])\n",
    "        clf.fit(X, y)\n",
    "        start_ts=time.time()\n",
    "        scores = cross_validate(clf, X, y, scoring=scorers, cv=5)    \n",
    "        scores_Acc = scores['test_Accuracy']                                                                                                                                    \n",
    "        print(name,\"ACC: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))                                                                                                    \n",
    "        scores_AUC= scores['test_roc_auc']                \n",
    "        print(name,\"AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))                           \n",
    "        print(\"CV Runtime:\", time.time()-start_ts)\n",
    "        model_results.append((name, scores_Acc.mean(), scores_Acc.std(), scores_AUC.mean(), scores_AUC.std(), time.time()-start_ts))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
